# -*- coding: utf-8 -*-
"""NTK_empirical_generalized.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/18-vCRoEqZVQKXG_G2IeobL4rkN6MjZzc

# Setup and Library Import
"""

import numpy as np
import random
import matplotlib.pyplot as plt
import math

# !pip -q install --upgrade pip # for Colab, comment when running locally

# for Colab, use older versions as newer ones produce jax/neural_tangent versions conflicts
# !pip install -q "jax[cuda12]==0.5.3" jaxlib==0.5.3
# !pip install -q neural_tangents==0.6.5

import neural_tangents as nt
from neural_tangents import stax

from jax import jit
from jax import numpy as jnp
from jax import random as jrand

import jax
jax.config.update("jax_default_matmul_precision", "float32")
import os
os.makedirs("img",exist_ok=True) # create folder for images if it does not already exist


"""# Helper Functions"""

# Util funcs

# use numpy instead of torch now
def input(gamma:float)->np.array:
  x=np.array([np.cos(gamma),np.sin(gamma)])
  return x.reshape(1,-1)


def DropConnect(width, rate, param):
   
    dense_init, dense_apply, kernel_fun = stax.Dense(width, parameterization = param)

    def init_fun(rng, input_shape):
        return dense_init(rng, input_shape)

    def apply_fun(params, inputs, rng=None, mode='train'):
        W, b = params
        if mode == 'train' and rng is not None:
            keep_prob = 1.0 - rate
            mask = random.bernoulli(rng, p=keep_prob, shape=W.shape)
            masked_W = W * mask / keep_prob
            masked_params = (masked_W, b)
        else:
            masked_params = params
        return dense_apply(masked_params, inputs)

    return init_fun, apply_fun, kernel_fun


def create_model(width, depth, seed, out_dim,activation,parameterization='ntk'):
  # parameterization should be 'ntk' or 'standard'
  # see here for parameterization documentation
  # https://neural-tangents.readthedocs.io/en/latest/_autosummary/neural_tangents.stax.Dense.html
  random.seed(seed)
  layer_list=[]
  for i in range(depth-1):
    layer_list.append(DropConnect(width, 0.0, parameterization))
    # layer_list.append(stax.Dense(width,parameterization=parameterization))
    layer_list.append(activation())
  layer_list.append(DropConnect(out_dim, 0.0, parameterization))
  init_fn, apply_fn, _ = stax.serial(*layer_list)
  model = dict(
      f=apply_fn,
      trace_axes=(),
      vmap_axes=0
  )
  _, params = init_fn(jrand.PRNGKey(seed), input(0).shape)
  return (model, params) # outputs interface directly with functions to get NTKs

# for comparison in Jacot plots, only works for 1d output for now
def create_infinite_width_kernel(depth,activation, seed, parameterization='ntk'):
  # parameterization should be 'ntk' or 'standard'
  # see here for parameterization documentation
  # https://neural-tangents.readthedocs.io/en/latest/_autosummary/neural_tangents.stax.Dense.html
  random.seed(seed)
  layer_list=[]
  for i in range(depth-1):
    layer_list.append(stax.Dense(1,parameterization=parameterization)) # width does not matter for infinite width
    layer_list.append(activation())
  layer_list.append(stax.Dense(4,parameterization=parameterization))
  _, _, kernel_fn = stax.serial(*layer_list)
  return kernel_fn

"""Very simple training function to see how the NTK looks before and after training. Needs to be updated to use stax instead of torch."""

def create_train_data(in_dim : int, out_dim : int, num_samples : int, input_range:tuple, output_range:tuple):
  import random
  tx = jnp.array([[random.uniform(input_range[0], input_range[1]) for i in range(in_dim)] for n in range(num_samples)])
  ty = jnp.array([[random.uniform(output_range[0], output_range[1]) for j in range(out_dim)] for n in range(num_samples)])
  return tx, ty
  
def train_model(model, epochs, x_train, y_train, batch_size=100):
  params = model[1]
  model_fwd = model[0]["f"]

  from jax import grad
  from jax.example_libraries import optimizers
  from jax.nn import log_softmax

  opt_init, opt_update, get_params = optimizers.adam(step_size=1e-3)
  opt_state = opt_init(params)

  def MSEloss(params, batch):
    inputs, targets = batch
    # print(inputs.shape)
    logits = model_fwd(params, inputs)
    return jnp.mean(jnp.square(logits - targets))
    

  def update(step, params, batch, lr=0.001):
    loss = MSEloss(params, batch)
    grads = grad(MSEloss)(params, batch)
    return opt_update(step, grads, opt_state), loss
  
  # 6. Training loop
  stepnum = 0
  # training per sample
  nBatches = len(x_train) // batch_size

  for epoch in range(epochs):
      # print("Epoch ", epoch)
      for i in range(nBatches):
          ll = i * batch_size
          hl = ll + batch_size
          batch = (x_train[ll:hl], y_train[ll:hl])
          opt_state, loss = update(epoch * nBatches + i, params, batch)
          print(loss)
          params = get_params(opt_state)
          stepnum += 1

  return (model[0], params)



def get_NTK(model_args,ref_input,input):
  kwargs=model_args[0] # outputs of create_model
  params=model_args[1]
  ntvp = jit(nt.empirical_ntk_fn(
    **kwargs, implementation=nt.NtkImplementation.NTK_VECTOR_PRODUCTS))
  NTK=ntvp(ref_input,input,params)
  return NTK


def doNTK(model_args, gamma_spacing:float=0.01):
  kwargs=model_args[0] # outputs of create_model
  params=model_args[1]
  gammas = np.arange(-1*np.pi, np.pi, gamma_spacing)

  inputs=[input(gamma) for gamma in gammas]
  inputs = np.stack(inputs).squeeze()

  ntvp = jit(nt.empirical_ntk_fn(
    **kwargs, implementation=nt.NtkImplementation.NTK_VECTOR_PRODUCTS))
  NTK_arr = ntvp(input(0.0),inputs,params)

  return gammas, NTK_arr.squeeze()

def doNTK_surface(model_args, numpts):
  kwargs=model_args[0] # outputs of create_model
  params=model_args[1]
  X = np.linspace(-1, 1, numpts, dtype=np.float32)
  Y = np.linspace(-1, 1, numpts, dtype=np.float32)
  gX, gY = np.meshgrid(X, Y)
  inputs=[np.array([gX.flatten()[i],gY.flatten()[i]]) for i in range(len(gX.flatten()))]
  inputs=np.stack(inputs)

  ntvp = jit(nt.empirical_ntk_fn(
  **kwargs, implementation=nt.NtkImplementation.NTK_VECTOR_PRODUCTS))

  gZ=ntvp(input(0.0),inputs,params)
  gZ=np.reshape(gZ,gX.shape)


  return gX, gY, gZ


def get_NTK_eigenvalues(model_args, input_1, input_2, return_NTK=False):
  # kwargs is optional arguments used only for printing latex table text
  NTK = get_NTK(model_args, input_1, input_2)
  lambdas,_ = np.linalg.eig(NTK)
  lambdas = np.abs(lambdas)
  condition_number = np.max(lambdas)/np.min(lambdas)
  return lambdas, NTK


def act(name):
  if name == "relu":
    return stax.Relu
  elif name == "gabor":
    return stax.Gabor
  elif name == "rbf":
    return stax.Rbf

def part1(config):
  vals = config["part1"]
  if (not vals["enable"]):
    return 
  ACTIVATIONS = vals['ACTIVATIONS']
  WIDTHS = vals['WIDTHS']
  COLORS = vals['COLORS']
  DEPTHS = vals['DEPTHS']
  SEED_LIST = vals['SEED_LIST']
  SURFACE_POINTS = vals["POINTS"]
  OUT_DIM = vals['OUT_DIM']

  for ACTIVATION_NAME in ACTIVATIONS:
    ACTIVATION = act(ACTIVATION_NAME)
    for DEPTH in DEPTHS:
      for i, WIDTH in enumerate(WIDTHS):
        for SEED in SEED_LIST:
          model = create_model(WIDTH, DEPTH, SEED, OUT_DIM, ACTIVATION) # use output_dim=1 for surface plots
          gap = 2.0 / SURFACE_POINTS
          X, Y = doNTK(model, gap)
          bY = [Y[n].item() for n in range(len(Y))]
          if SEED == SEED_LIST[0]:
            plt.plot(X, bY,label=f"Width={WIDTH}", color = COLORS[i])
          else:
            plt.plot(X, bY, color = COLORS[i])
      # plot infinite width, takes extra time but is interesting result
      k_fn = create_infinite_width_kernel(DEPTH, ACTIVATION, SEED)
      gammas = np.arange(-1*np.pi, np.pi, 0.05)
      ntks = [k_fn(input(0),input(gamma),'ntk') for gamma in gammas]
      ntks = np.stack(ntks).squeeze()
      plt.plot(gammas,ntks,label="Infinite width",color='black',linewidth=3)

      s=f"img/p1_{str(ACTIVATION.__name__)}_plot_depth{DEPTH}"
      plt.xlabel("gamma")
      plt.ylabel("NTK")
      plt.legend()
      plt.savefig(s+".png")
      print("Saved with width "+str(WIDTH))
      plt.clf()


def plothist(vals, bins):
  histy, binedges = np.histogram(vals, bins = bins)
  binsx = [(binedges[i + 1] + binedges[i]) * 0.5 for i in range(len(binedges) - 1)]
  plt.plot(binsx, histy)
  # plt.xticks(binedges)

def part2(config):
  vals = config["part2"]
  if (not vals["enable"]):
    return
  ACTIVATION_NAMES = vals["ACTIVATIONS"]
  WIDTHS = vals['WIDTHS']
  DEPTHS = vals['DEPTHS']
  SEEDS = vals['SEED_LIST']
  OUT_DIMS = vals['OUT_DIMS']
  EPOCHS = vals["EPOCHS"]
  NUM_TRAINING_POINTS = vals["TRAINING_POINTS"]

  for ACTIVATION_NAME in ACTIVATION_NAMES:
    ACTIVATION = act(ACTIVATION_NAME)
    for OUT_DIM in OUT_DIMS:
      x, y = create_train_data(2, OUT_DIM, 100, (-1.0, 1.0), (-2.0, 2.0))
      for DEPTH in DEPTHS:
        for WIDTH in WIDTHS:
          lambdas_w1 = []
          lambdas_w2 = []
          print("depth = ", DEPTH, "width = ", WIDTH)
          for SEED in SEEDS:
            print("seed = ", SEED)
            model = create_model(WIDTH, DEPTH, SEED, OUT_DIM, ACTIVATION)
            lambdas_pre, NTK_PRE = get_NTK_eigenvalues(model, input(0.0), input(1.0))
            lambdas_w1.append(lambdas_pre)
            
            model = train_model(model, EPOCHS, x, y, batch_size = 50)
           
            lambdas_post, NTK_POST = get_NTK_eigenvalues(model, input(0.0), input(1.0))
            lambdas_w2.append(lambdas_post)
            for item in [*model]:
              del item #            del model[1]

          lambdas_w1 = np.concatenate(lambdas_w1).flatten()
          lambdas_w2 = np.concatenate(lambdas_w2).flatten()
          xl = min(min(lambdas_w2), min(lambdas_w1))
          xh = max(max(lambdas_w2), max(lambdas_w1))
          # print(lambdas_w1)
          # print(lambdas_w2)
          plt.subplot(2, 1, 1)
          # plothist(lambdas_w1, 10)
          plt.xlim([xl, xh])
          plt.hist(lambdas_w1,label=f"Width={WIDTH}", bins=10, alpha = 1.0 / len(WIDTHS))
          plt.subplot(2, 1, 2)
          plt.xlim([xl, xh])
          plt.hist(lambdas_w2,label=f"Width={WIDTH}", bins=10, alpha = 1.0 / len(WIDTHS))
          # plothist(lambdas_w2, 10)
        
        s = f"img/p2_Act_{ACTIVATION.__name__}_depth_{DEPTH}_outdim_{OUT_DIM}"
        plt.suptitle(f"Depth={DEPTH}, Output dimension={OUT_DIM}, Activation={ACTIVATION_NAME}")
        plt.subplot(2, 1, 1)
        plt.legend()
        plt.subplot(2, 1, 2)
        plt.legend()
        plt.savefig(s+"_eigen.png")
        plt.clf()
      del x
      del y
          

          
          
  pass


def part3(config):
  vals = config["part3"]
  if (not vals["enable"]):
    return
  ACTIVATION_NAMES = vals["ACTIVATIONS"]
  WIDTHS = vals['WIDTHS']
  DEPTHS = vals['DEPTHS']
  SEEDS = vals['SEED_LIST']
  OUT_DIMS = vals['OUT_DIMS']
  POINTS = 10
  spacing = 0.2
  for ACTIVATION_NAME in ACTIVATION_NAMES:
    ACTIVATION = act(ACTIVATION_NAME)
    for OUT_DIM in OUT_DIMS:
      for DEPTH in DEPTHS:
        for WIDTH in WIDTHS:
          lambdas_W=[]
          print(f"Starting width={WIDTH}")
          for SEED in SEEDS:
            mod = create_model(WIDTH, DEPTH, SEED, OUT_DIM, ACTIVATION)
            # print(lambdas_inf)
            # for n in range(POINTS):
            theta = 1 #(-1 + spacing * n)*(3.14159)
            lambdas, ntk = get_NTK_eigenvalues(mod,input(gamma=0),input(gamma=theta))
            
            lambdas_W.append(lambdas)

          lambdas_W = np.concatenate(lambdas_W).flatten()
          plt.hist(lambdas_W,label=f"Width={WIDTH}", bins=20)
        s = f"img/p3_Act_{ACTIVATION.__name__}_depth_{DEPTH}_outdim_{OUT_DIM}"
        plt.title(f"Depth={DEPTH}, Output dimension={OUT_DIM}, Activation={ACTIVATION_NAME}")
        plt.legend()
        plt.savefig(s+"_eigen.png")
        plt.clf()
        


import sys
import json
with open(sys.argv[1]) as f:
  cfg = json.load(f)
  part1(cfg)
  part2(cfg)
  part3(cfg)
