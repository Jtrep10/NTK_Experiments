# -*- coding: utf-8 -*-
"""NTK_empirical_generalized.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/18-vCRoEqZVQKXG_G2IeobL4rkN6MjZzc

# Setup and Library Import
"""

import numpy as np
import random
import matplotlib.pyplot as plt
import math

# !pip -q install --upgrade pip # for Colab, comment when running locally

# for Colab, use older versions as newer ones produce jax/neural_tangent versions conflicts
# !pip install -q "jax[cuda12]==0.5.3" jaxlib==0.5.3
# !pip install -q neural_tangents==0.6.5

import neural_tangents as nt
from neural_tangents import stax

from jax import jit
from jax import numpy as jnp
from jax import random as jrand

import jax
jax.config.update("jax_default_matmul_precision", "float32")
import os
os.makedirs("img",exist_ok=True) # create folder for images if it does not already exist


"""# Helper Functions"""

# Util funcs

# use numpy instead of torch now
def input(gamma:float)->np.array:
  x=np.array([np.cos(gamma),np.sin(gamma)])
  return x.reshape(1,-1)


def create_model(width, depth, seed, out_dim,activation,parameterization='ntk'):
  # parameterization should be 'ntk' or 'standard'
  # see here for parameterization documentation
  # https://neural-tangents.readthedocs.io/en/latest/_autosummary/neural_tangents.stax.Dense.html
  random.seed(seed)
  layer_list=[]
  for i in range(depth-1):
    layer_list.append(stax.Dense(width,parameterization=parameterization))
    layer_list.append(activation())
  layer_list.append(stax.Dense(out_dim,parameterization=parameterization))
  init_fn, apply_fn, _ = stax.serial(*layer_list)
  model = dict(
      f=apply_fn,
      trace_axes=(),
      vmap_axes=0
  )
  _, params = init_fn(jrand.PRNGKey(seed), input(0).shape)
  return (model, params) # outputs interface directly with functions to get NTKs

# for comparison in Jacot plots, only works for 1d output for now
def create_infinite_width_kernel(depth,activation, seed, parameterization='ntk'):
  # parameterization should be 'ntk' or 'standard'
  # see here for parameterization documentation
  # https://neural-tangents.readthedocs.io/en/latest/_autosummary/neural_tangents.stax.Dense.html
  random.seed(seed)
  layer_list=[]
  for i in range(depth-1):
    layer_list.append(stax.Dense(1,parameterization=parameterization)) # width does not matter for infinite width
    layer_list.append(activation())
  layer_list.append(stax.Dense(1,parameterization=parameterization))
  _, _, kernel_fn = stax.serial(*layer_list)
  return kernel_fn

"""Very simple training function to see how the NTK looks before and after training. Needs to be updated to use stax instead of torch."""

def create_train_data(in_dim : int, out_dim : int, num_samples : int, input_range:tuple, output_range:tuple):
  import random
  tx = jnp.array([[random.uniform(input_range[0], input_range[1]) for i in range(in_dim)] for n in range(num_samples)])
  ty = jnp.array([[random.uniform(output_range[0], output_range[1]) for j in range(out_dim)] for n in range(num_samples)])
  return tx, ty
  
def train_model(model, epochs, x_train, y_train, batch_size=100):
  params = model[1]
  model_fwd = model[0]["f"]

  from jax import grad
  from jax.example_libraries import optimizers
  from jax.nn import log_softmax

  opt_init, opt_update, get_params = optimizers.adam(step_size=1e-3)
  opt_state = opt_init(params)

  def CEloss(params, batch):
    inputs, targets = batch
    logits = model_fwd(params, inputs)
    log_probs = log_softmax(logits)
    return -jnp.mean(jnp.sum(targets * log_probs, axis=1))  # cross-entropy

  def update(step, params, batch, lr=0.001):
   
    grads = grad(CEloss)(params, batch)
    return opt_update(step, grads, opt_state)
  
  # 6. Training loop
  stepnum = 0
  # training per sample
  nBatches = len(x_train) // batch_size

  for epoch in range(epochs):
      print("Epoch ", epoch)
      for i in range(nBatches):
          ll = i * batch_size
          hl = ll + batch_size
          batch = (x_train[ll:hl], y_train[ll:hl])
          opt_state = update(epoch * nBatches + i, params, batch)
          params = get_params(opt_state)
          stepnum += 1

  return (model[0], params)



def get_NTK(model_args,ref_input,input):
  kwargs=model_args[0] # outputs of create_model
  params=model_args[1]
  ntvp = jit(nt.empirical_ntk_fn(
    **kwargs, implementation=nt.NtkImplementation.NTK_VECTOR_PRODUCTS))
  NTK=ntvp(ref_input,input,params)
  return NTK


def doNTK(model_args, gamma_spacing:float=0.01):
  kwargs=model_args[0] # outputs of create_model
  params=model_args[1]
  gammas = np.arange(-1*np.pi, np.pi, gamma_spacing)

  inputs=[input(gamma) for gamma in gammas]
  inputs = np.stack(inputs).squeeze()

  ntvp = jit(nt.empirical_ntk_fn(
    **kwargs, implementation=nt.NtkImplementation.NTK_VECTOR_PRODUCTS))
  NTK_arr = ntvp(input(0.0),inputs,params)

  return gammas, NTK_arr.squeeze()

def doNTK_surface(model_args, numpts):
  kwargs=model_args[0] # outputs of create_model
  params=model_args[1]
  X = np.linspace(-1, 1, numpts, dtype=np.float32)
  Y = np.linspace(-1, 1, numpts, dtype=np.float32)
  gX, gY = np.meshgrid(X, Y)
  inputs=[np.array([gX.flatten()[i],gY.flatten()[i]]) for i in range(len(gX.flatten()))]
  inputs=np.stack(inputs)

  ntvp = jit(nt.empirical_ntk_fn(
  **kwargs, implementation=nt.NtkImplementation.NTK_VECTOR_PRODUCTS))

  gZ=ntvp(input(0.0),inputs,params)
  gZ=np.reshape(gZ,gX.shape)


  return gX, gY, gZ


def get_NTK_eigenvalues(model_args, input_1, input_2, return_NTK=False):
  # kwargs is optional arguments used only for printing latex table text
  NTK = get_NTK(model_args, input_1, input_2)
  lambdas,_ = np.linalg.eig(NTK)
  lambdas = np.abs(lambdas)
  condition_number = np.max(lambdas)/np.min(lambdas)
  return lambdas, NTK


def act(name):
  if name == "relu":
    return stax.Relu
  elif name == "gabor":
    return stax.Gabor
  elif name == "rbf":
    return stax.Rbf

def part1(config):
  vals = config["part1"]
  if (not vals["enable"]):
    return 
  ACTIVATIONS = vals['ACTIVATIONS']
  WIDTHS = vals['WIDTHS']
  COLORS = vals['COLORS']
  DEPTHS = vals['DEPTHS']
  SEED_LIST = vals['SEED_LIST']
  SURFACE_POINTS = vals["POINTS"]
  OUT_DIM = vals['OUT_DIM']

  for ACTIVATION_NAME in ACTIVATIONS:
    ACTIVATION = act(ACTIVATION_NAME)
    for DEPTH in DEPTHS:
      for i, WIDTH in enumerate(WIDTHS):
        for SEED in SEED_LIST:
          model = create_model(WIDTH, DEPTH, SEED, OUT_DIM, ACTIVATION) # use output_dim=1 for surface plots
          gap = 2.0 / SURFACE_POINTS
          X, Y = doNTK(model, gap)
          bY = [Y[n].item() for n in range(len(Y))]
          if SEED == SEED_LIST[0]:
            plt.plot(X, bY,label=f"Width={WIDTH}", color = COLORS[i])
          else:
            plt.plot(X, bY, color = COLORS[i])
      # plot infinite width, takes extra time but is interesting result
      k_fn = create_infinite_width_kernel(DEPTH, ACTIVATION, SEED)
      gammas = np.arange(-1*np.pi, np.pi, 0.05)
      ntks = [k_fn(input(0),input(gamma),'ntk') for gamma in gammas]
      ntks = np.stack(ntks).squeeze()
      plt.plot(gammas,ntks,label="Infinite width",color='black',linewidth=3)

      s=f"img/p1_{str(ACTIVATION.__name__)}_plot_depth{DEPTH}"
      plt.xlabel("gamma")
      plt.ylabel("NTK")
      plt.legend()
      plt.savefig(s+".png")
      print("Saved with width "+str(WIDTH))
      plt.clf()

def part2(config):
  vals = config["part2"]
  if (vals["enable"]):
    return
  pass

def part3(config):
  vals = config["part3"]
  if (not vals["enable"]):
    return
  ACTIVATION_NAMES = vals["ACTIVATIONS"]
  WIDTHS = vals['WIDTHS']
  DEPTHS = vals['DEPTHS']
  SEEDS = vals['SEED_LIST']
  OUT_DIMS = vals['OUT_DIMS']

  for ACTIVATION_NAME in ACTIVATION_NAMES:
    ACTIVATION = act(ACTIVATION_NAME)
    for OUT_DIM in OUT_DIMS:
      for DEPTH in DEPTHS:
        for WIDTH in WIDTHS:
          lambdas_W=[]
          print(f"Starting width={WIDTH}")
          for SEED in SEEDS:
            mod = create_model(WIDTH, DEPTH, SEED, OUT_DIM, ACTIVATION)
            lambdas, ntk = get_NTK_eigenvalues(mod,input(gamma=0),input(gamma=1)) # can change gammas for input
            print(lambdas)
            lambdas_W.append(lambdas)
          lambdas_W=np.concatenate(lambdas,axis=0).squeeze() # remove extra dimension          
          plt.hist(lambdas_W,label=f"Width={WIDTH}")
        s = f"img/p3_Act_{ACTIVATION.__name__}_depth_{DEPTH}_outdim_{OUT_DIM}"
        plt.title(f"Depth={DEPTH}, Output dimension={OUT_DIM}, Activation={ACTIVATION_NAME}")
        plt.legend()
        plt.savefig(s+"_eigen.png")
        plt.clf()
        


import sys
import json
with open(sys.argv[1]) as f:
  cfg = json.load(f)
  part1(cfg)
  part2(cfg)
  part3(cfg)